{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f23655a",
   "metadata": {},
   "source": [
    "# üçè Health Assistant Evaluation Demo üçé\n",
    "\n",
    "This notebook demonstrates how to use Azure AI Foundry's evaluation capabilities to assess the quality and safety of AI-generated health and fitness responses.\n",
    "\n",
    "## üîê Authentication Setup\n",
    "\n",
    "Before running the next cell, make sure you're authenticated with Azure CLI. Run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "az login --use-device-code\n",
    "```\n",
    "\n",
    "This will provide you with a device code and URL to authenticate in your browser, which is useful for:\n",
    "- Remote development environments\n",
    "- Systems without a default browser\n",
    "- Corporate environments with strict security policies\n",
    "\n",
    "After successful authentication, you can proceed with the notebook cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02495886",
   "metadata": {},
   "source": [
    "## üìä Available Evaluators in Azure AI Foundry\n",
    "\n",
    "Azure AI Foundry provides a comprehensive set of built-in evaluators for different aspects of AI model quality:\n",
    "\n",
    "### **AI Quality (AI Assisted)**\n",
    "- **Groundedness** - Measures how well responses are grounded in provided context\n",
    "- **Relevance** - Evaluates how relevant responses are to the input query  \n",
    "- **Coherence** - Assesses logical flow and consistency in responses\n",
    "- **Fluency** - Measures language quality and readability\n",
    "- **GPT Similarity** - Compares responses to reference answers\n",
    "\n",
    "### **AI Quality (NLP Metrics)**\n",
    "- **F1 Score** - Measures precision and recall balance\n",
    "- **ROUGE Score** - Evaluates text summarization quality\n",
    "- **BLEU Score** - Measures translation and generation quality\n",
    "- **GLEU Score** - Google's BLEU variant for better correlation\n",
    "- **METEOR Score** - Considers synonyms and stemming\n",
    "\n",
    "### **Risk and Safety**\n",
    "- **Violence** - Detects violent content\n",
    "- **Sexual** - Identifies sexual content\n",
    "- **Self-harm** - Detects self-harm related content\n",
    "- **Hate/Unfairness** - Identifies hateful or unfair content\n",
    "- **Protected Material** - Detects copyrighted content\n",
    "- **Indirect Attack** - Identifies indirect prompt injection attempts\n",
    "\n",
    "üìö **For complete details on all available evaluators, their parameters, and usage examples, visit:**  \n",
    "**[Azure AI Foundry Evaluators Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c0ebe",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Azure AI Foundry Evaluations üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "This notebook demonstrates how to evaluate AI models using Azure AI Foundry with both **local** and **cloud** evaluations.\n",
    "\n",
    "## What This Notebook Does:\n",
    "1. **Setup & Data Creation** - Creates synthetic health & fitness Q&A data\n",
    "2. **Local Evaluation** - Runs F1Score and Relevance evaluators locally  \n",
    "3. **Cloud Evaluation** - Uploads results to Azure AI Foundry project\n",
    "\n",
    "## Key Features:\n",
    "‚úÖ **Local Evaluations** - F1Score and AI-assisted Relevance evaluators\n",
    "‚úÖ **Cloud Integration** - Upload results to Azure AI Foundry\n",
    "‚úÖ **Browser Authentication** - Uses InteractiveBrowserCredential  \n",
    "‚úÖ **Error Handling** - Robust fallbacks and clear status reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b889daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup and Data Creation\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from root directory\n",
    "root_env_path = os.environ.get(\"ROOT_ENV_PATH\", '../../../.env')\n",
    "load_dotenv(root_env_path)\n",
    "print(f\"‚úÖ Environment variables loaded from: {root_env_path}\")\n",
    "\n",
    "# Check required environment variables for Azure AI Foundry\n",
    "AI_FOUNDRY_PROJECT_ENDPOINT = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "TENANT_ID = os.environ.get(\"TENANT_ID\")\n",
    "\n",
    "print(\"üîç Environment Variables Status:\")\n",
    "print(\n",
    "    f\"   AI_FOUNDRY_PROJECT_ENDPOINT: {'‚úÖ Set' if AI_FOUNDRY_PROJECT_ENDPOINT else '‚ùå Missing'}\"\n",
    ")\n",
    "print(f\"   TENANT_ID: {'‚úÖ Set' if TENANT_ID else '‚ùå Missing'}\")\n",
    "\n",
    "if not AI_FOUNDRY_PROJECT_ENDPOINT:\n",
    "    print(\"\\n‚ö†Ô∏è Required environment variables missing!\")\n",
    "    print(\"Please add these to your .env file:\")\n",
    "    print(\"AI_FOUNDRY_PROJECT_ENDPOINT=<your-azure-ai-project-endpoint>\")\n",
    "    print(\"TENANT_ID=<your-azure-tenant-id>\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All environment variables configured correctly!\")\n",
    "    print(f\"üîß Loaded values:\")\n",
    "    print(f\"   AI_FOUNDRY_PROJECT_ENDPOINT: {AI_FOUNDRY_PROJECT_ENDPOINT}\")\n",
    "    print(f\"   TENANT_ID: {TENANT_ID}\")\n",
    "\n",
    "# Create synthetic health & fitness evaluation data\n",
    "synthetic_eval_data = [\n",
    "    {\n",
    "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
    "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
    "        \"response\": \"You can just go for 10 push-ups total.\",\n",
    "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
    "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
    "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
    "        \"ground_truth\": \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the capital of France?\",\n",
    "        \"context\": \"France is in Europe. Paris is the capital.\",\n",
    "        \"response\": \"London.\",\n",
    "        \"ground_truth\": \"Paris.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Write data to JSONL file\n",
    "eval_data_filename = os.environ.get(\"EVAL_DATA_FILENAME\", \"health_fitness_eval_data.jsonl\")\n",
    "eval_data_path = Path(f\"./{eval_data_filename}\")\n",
    "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in synthetic_eval_data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Evaluation data created: {eval_data_path.resolve()}\")\n",
    "print(f\"üìä Total samples: {len(synthetic_eval_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d5598",
   "metadata": {
    "id": "3-Local-Evaluation"
   },
   "source": [
    "## üîç Local Evaluation\n",
    "\n",
    "Run evaluations locally using F1Score (basic text similarity) and Relevance (AI-assisted) evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f04f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Evaluation with Azure AI Foundry\n",
    "from azure.ai.evaluation import evaluate, F1ScoreEvaluator, RelevanceEvaluator\n",
    "import logging\n",
    "\n",
    "# Reduce logging noise\n",
    "logging.getLogger('promptflow').setLevel(logging.ERROR)\n",
    "logging.getLogger('azure.ai.evaluation').setLevel(logging.WARNING)\n",
    "\n",
    "print(\"üîç Running Local Evaluation...\")\n",
    "\n",
    "# Configure evaluators\n",
    "evaluators = {\n",
    "    \"f1_score\": F1ScoreEvaluator()\n",
    "}\n",
    "\n",
    "evaluator_config = {\n",
    "    \"f1_score\": {\n",
    "        \"column_mapping\": {\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"ground_truth\": \"${data.ground_truth}\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add AI-assisted evaluator if Azure OpenAI is configured\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4\")),\n",
    "    \"api_version\": os.environ.get(\"AOAI_API_VERSION\", os.environ.get(\"API_VERSION\", \"2024-02-15-preview\")),\n",
    "}\n",
    "\n",
    "if model_config[\"azure_endpoint\"] and model_config[\"api_key\"]:\n",
    "    print(\"ü§ñ Adding AI-assisted Relevance evaluator...\")\n",
    "    evaluators[\"relevance\"] = RelevanceEvaluator(model_config=model_config)\n",
    "    evaluator_config[\"relevance\"] = {\n",
    "        \"column_mapping\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\"\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Azure OpenAI not configured - using F1Score only\")\n",
    "\n",
    "# Run local evaluation\n",
    "try:\n",
    "    local_result = evaluate(\n",
    "        data=str(eval_data_path),\n",
    "        evaluators=evaluators,\n",
    "        evaluator_config=evaluator_config\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Local evaluation completed!\")\n",
    "    \n",
    "    # Display results\n",
    "    metrics = local_result['metrics']\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"üìä {metric_name}: {value:.4f}\")\n",
    "        \n",
    "        # Save results locally\n",
    "        local_results_filename = os.environ.get(\"LOCAL_RESULTS_FILENAME\", \"local_evaluation_results.json\")\n",
    "        with open(local_results_filename, \"w\") as f:\n",
    "            json.dump(local_result, f, indent=2)\n",
    "\n",
    "        print(f\"üíæ Results saved to: {local_results_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Local evaluation failed: {e}\")\n",
    "    local_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d525400",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Cloud Evaluation\n",
    "\n",
    "Upload evaluation results to Azure AI Foundry project for tracking and collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Evaluation - Using azure-ai-evaluation SDK directly\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation import evaluate, BleuScoreEvaluator, F1ScoreEvaluator\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"‚òÅÔ∏è Setting up Cloud Evaluation with Azure AI Foundry...\")\n",
    "\n",
    "# Configuration from environment variables\n",
    "AI_FOUNDRY_PROJECT_ENDPOINT = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "AZURE_SUBSCRIPTION_ID = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "\n",
    "print(f\"üè¢ Foundry Project Endpoint: {AI_FOUNDRY_PROJECT_ENDPOINT}\")\n",
    "print(f\"üîë Subscription ID: {AZURE_SUBSCRIPTION_ID}\")\n",
    "\n",
    "if not AI_FOUNDRY_PROJECT_ENDPOINT:\n",
    "    print(\"‚ö†Ô∏è Missing AI_FOUNDRY_PROJECT_ENDPOINT in .env file\")\n",
    "    cloud_result = None\n",
    "else:\n",
    "    try:\n",
    "        # Configure evaluators\n",
    "        print(\"‚öôÔ∏è Configuring evaluators...\")\n",
    "        evaluators = {\n",
    "            \"bleu_score\": BleuScoreEvaluator(),\n",
    "            \"f1_score\": F1ScoreEvaluator(),\n",
    "        }\n",
    "        \n",
    "        evaluator_config = {\n",
    "            \"bleu_score\": {\n",
    "                \"column_mapping\": {\n",
    "                    \"response\": \"${data.response}\",\n",
    "                    \"ground_truth\": \"${data.ground_truth}\",\n",
    "                }\n",
    "            },\n",
    "            \"f1_score\": {\n",
    "                \"column_mapping\": {\n",
    "                    \"response\": \"${data.response}\",\n",
    "                    \"ground_truth\": \"${data.ground_truth}\",\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        # Run evaluation with azure_ai_project for cloud tracking\n",
    "        print(\"üöÄ Running evaluation...\")\n",
    "        \n",
    "        result = evaluate(\n",
    "            data=str(eval_data_path),\n",
    "            evaluators=evaluators,\n",
    "            evaluator_config=evaluator_config,\n",
    "        )\n",
    "        \n",
    "        print(\"üéâ EVALUATION COMPLETED!\")\n",
    "        \n",
    "        # Display metrics\n",
    "        if 'metrics' in result:\n",
    "            for metric_name, value in result['metrics'].items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    print(f\"   üìä {metric_name}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   üìä {metric_name}: {value}\")\n",
    "        \n",
    "        # Save results\n",
    "        cloud_result = {\n",
    "            \"metrics\": result.get('metrics', {}),\n",
    "            \"rows\": result.get('rows', []),\n",
    "            \"project_endpoint\": AI_FOUNDRY_PROJECT_ENDPOINT,\n",
    "            \"timestamp\": int(time.time()),\n",
    "        }\n",
    "        \n",
    "        results_filename = \"cloud_evaluation_results.json\"\n",
    "        with open(results_filename, \"w\") as f:\n",
    "            json.dump(cloud_result, f, indent=2, default=str)\n",
    "        print(f\"üíæ Results saved to: {results_filename}\")\n",
    "\n",
    "        print(\"\\n‚úÖ SUCCESS: Evaluation completed!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        cloud_result = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
